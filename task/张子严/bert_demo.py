# -*- coding: utf-8 -*-
"""bert_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vg7ClGrd9eeN8YGdsHq_O86PPmI9e5dW
"""

! pip install transformers
import torch
import time
from transformers import BertModel, BertTokenizer, BertConfig
import pandas as pd
import numpy as np
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import *


bert_path = '/bert_model.ckpt.data-00000-of-00001' # The path that stored the pre-trained model
train_path = '/train.txt'
tokenizer = BertTokenizer(vocab_file='/vocab.txt') # initialize the tokenizer

"""# formatting the input"""

input_id = [] # this list stores the chars' id
segment_id = [] # this list stores the segments' id
input_mask = [] 
label = []
max_length = 32

with open("/train.txt", encoding='utf-8') as f:
    for l in f: 
        x, y = l.strip().split('\t') # we have two parts inside this file, content (x) and weight (y)
        x = tokenizer.tokenize(x)
        tokens = ["[CLS]"] + x + ["[SEP]"] # we only set the content as the token
        tokenizer.encode()
        ids = tokenizer.convert_tokens_to_ids(tokens)
        types = [0] *(len(ids))
        masks = [1] * len(ids)
        if len(ids) < max_length: # resize input strings to make them at a same length
            types = types + [1] * (max_length- len(ids)) 
            masks = masks + [0] * (max_length - len(ids))
            ids = ids + [0] * (max_length - len(ids))
        else:
            types = types[:max_length]
            masks = masks[:max_length]
            ids = ids[:max_length]
        input_id.append(ids)
        segment_id.append(types)
        input_mask.append(masks)
        # print(len(ids), len(masks), len(types)) easy to crush
        label.append([int(y)])

